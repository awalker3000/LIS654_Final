<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE = edge">
  <meta name="viewport" content="width = device-width, initial-scale = 1">
  <title>Web Archiving for Art History</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
</head>

<body>
  <div class="jumbotron">
    <div class="container">
      <h1>Archiving Digital Art Resources</h1>
      <p>Web Archiving and Art History</p>
    </div>
  </div>
  <nav class="navbar navbar-expand-md bg-dark navbar-dark">
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
    <span class="navbar-toggler-icon"></span>
  </button>
    <div class="collapse navbar-collapse" id="collapsibleNavbar">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="whyArchive.html">Why Archive?</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="projects.html">Projects</a>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" id="navbardrop" data-toggle="dropdown">
        Try it yourself
        </a>
            <div class="dropdown-menu bg-dark">
              <a class="dropdown-item" href="planning.html">Project Planning</a>
              <a class="dropdown-item" href="process.html">Process of archiving a website</a>
              <a class="dropdown-item" href="tools.html">Experiments with Archiving Tools</a>
            </div>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="archivability.html">Improving Website Archivability</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="aboutUs.html">About Us</a>
          </li>
        </ul>
    </div>
  </nav>
  <div class="container">
    <h2>Experiments with Archiving Tools</h2>
  </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <p>Archive-It is the tool most used by libraries and museums to create curated web archives. It offers support, connection to other institutional web archival collections, storage, and an all-in-one functionality.</p>
        <p>But what if you’re interested in creating a web archive on your own, your institution can’t afford subscription-based tools, or you simply want to explore what’s out there? Luckily there are many free options for web archiving, many of them also
          open-source and extensively documented, and you don’t have to be a tech genius to figure them out. You do, however, need to be familiar with the steps taken to archive a website. If you’re brand new to web archiving, check out my <a href="process.html">process page</a>
          before diving in on your own.</p>
        <p>To explore the viability of these tools, I chose a website already archived by an institution using Archive-It, and compared my freely-acquired results with theirs. The experiments below will give you a just a taste of the possibilities and
          difficulties in web archiving, and hopefully encourage you to experiment on your own.</p>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <h3>Acquisition</h3>
        <hr class="pink">
        <h5>Archive-It</h5><p><a href="https://archive-it.org/organizations/484a" target="_blank">NYARC</a> has been <a href="https://wayback.archive-it.org/4847/*/http://bortolamigallery.com/" target="_blank"> archiving</a> the website of the <a href="http://bortolamigallery.com/" target="_blank">Bortolami Gallery</a> since 2014.
          Archive-It uses a combination of tools, including brozzler and youtube-dl, to crawl pages and save them as WARC files.
          Looking at NYARC’s latest capture of the gallery website on March 28th, 2018, it appears that all internal links and files on the site have been captured.
          The pages for each artist are accessible, and the PDFs attached can be downloaded.</p>
        </div>
      </div>
    </div>
            <div class="container">
              <div class="row">
                <div class="col-md-6">
          <img src="tools-1-1.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="NYARC's archived version of the Bortolami Gallery website">
        </div>
              <div class="col-md-6">
          <img src="tools-1-2.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="NYARC's archived version of the Bortolami Gallery website"><br>
        </div>
      </div>
    </div>
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <p>It stops short of saving their social media sites linked to at the bottom of each page, however.
            This is because Archive-It allows you to have control over scoping - it appears the archivist decided to not save those pages in this particular instance.</p><br>
          <img src="tools-1-3.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="NYARC's archived version of the Bortolami Gallery website">
      </div>
    </div>
  </div>
<div class="container">
  <div class="row">
    <div class="col-md-12">
      <hr class="pink">
      <h5>Internet Archive</h5>
      <p>Of course, the easiest way to get involved is to contribute to existing web archives.
        The Internet Archive offers the “save page now” button that allow you to activate a crawl with one click.</p>
        <p>You can also save pages to the Wayback Machine using their:</p>
        <ul>
          <li><a href="https://chrome.google.com/webstore/detail/wayback-machine/fpnmgdkabkmnadcjpehmlllkndpkmiak" target="_blank">Google Chrome extension</a></li>
          <li><a href="https://addons.mozilla.org/en-US/firefox/addon/wayback-machine_new/" target="_blank">Firefox add-on</a></li>
          <li><a href="https://safari-extensions.apple.com/details/?id=archive.org.waybackmachine-ZSFX78H3ZT" target="_blank">Safari extension</a></li>
          <li><a href="https://itunes.apple.com/us/app/wayback-machine/id1201888313" target="_blank">iPhone app</a></li>
          <li><a href="https://play.google.com/store/apps/details?id=com.archive.waybackmachine" target="_blank">Android app</a></li>
        </ul>
      </div>
    </div>
  </div>
        <div class="container">
          <div class="row">
            <div class="col-md-6 vertical-center">
        <img src="tools-2-1.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="The 'save page now' feature of the Internet Archive">
      </div>
        <div class="col-md-6">
          <img src="tools-2-2.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="A website being saved with the 'save page now' feature">
    </div>
  </div>
  </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <p>The downsides?</p>
        <ul>
<li>They don’t work on sites that don’t allow crawlers</li>
<li>You have no control over the scope, so you may end up with terrible or incomplete captures - this is a fairly simple site, so my capture is more or less indistinguishable from the NYARC capture</li>
<li>General public (including you) don’t have access to the ARC or WARC file, so you cannot save and store the data yourself</li>
<li>Your crawl will not display as having been saved by you or as a part of your collection. My capture, for example, is in the “Live Web Proxy Crawls Collection”</li>
</ul>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <hr class="pink">
        <h5>Webrecorder</h5>
        <p><a href="https://webrecorder.io/" target="_blank">Webrecorder</a> is a part of a suite of web archiving software developed by Rhizome.
          Learn more about their web archiving efforts on our <a href="projects.html">projects page</a>.
          Once you’ve established a free profile, you can record, store, and replay versions of websites.</p>
            <div class="text-center">
            <img src="tools-3-1.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="Screenshot of Webrecorder default collection">
            <figcaption class="figure-caption">Starting my first collection using Webrecorder</figcaption><br></div>
          <p>Rather than crawl a website by automatically following all possible links, Webrecorder records the actions you take in your browser.
            This allows you to target specific pages, files, and media on a site, including those that would otherwise be unfindable or difficult to capture.
            This makes Webrecorder a great tool for saving audiovisual, password protected, or form driven content as the scope is determined simply by the paths you take.</p>
            <p>It does, however, involve a lot of clicking on your part.</p>
          </div>
        </div>
      </div>
      <div class="container">
        <div class="row">
          <div class="col-md-6">
            <div class="text-center">
              <img src="tools-3-2.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="Screenshot of Webrecorder recording session">
              <figcaption class="figure-caption">Navigating through the gallery website during a recording session</figcaption><br></div>
            </div>
            <div class="col-md-6">
              <div class="text-center">
                  <img src="tools-3-3.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="Screenshot of Webrecorder collection page">
                  <figcaption class="figure-caption">My completed recording in my collection page</figcaption><br></div>
                </div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-md-12">
              <p>Should you find you want to add pages to a recording session, you can try to “patch” a capture later, but I found it easier to try to capture everything I wanted all at once.
                Each capture can be named and added to your collections, which can be made private or public.
                You can also download the WARC files of the sites you capture and save them yourself.</p>
            </div>
          </div>
        </div>
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <hr class="pink">
              <h5>WARCreate and WAIL</h5>
              <p><a href="http://warcreate.com/" target="_blank">WARCreate</a> is another Google Chrome extension option for archiving websites.
                It allows you to generate WARC files from the websites you visit.
                However, I couldn’t get it to work! I suspect being an open-source Google Chrome browser extension created and maintained mainly by one person (Mat Kelly) is a part of the problem
                - Chrome is updated very frequently, so ensuring an extension’s compatibility with these updates involves serious maintenance.
                Its <a href="https://github.com/machawk1/warcreate" target="_blank">github page</a> does mention that it has gone through various release and retraction periods, so I look forward to trying it again in the future.</p>
                <p><a href="http://machawk1.github.io/wail/" target="_blank">WAIL</a>, or Web Archiving Integration Layer, is another tool built by Mat Kelly, and more of an all-in-one application.
                  It uses Heritrix to obtain WARC files.
                  Again there are signs that its ongoing development is the mostly the work of one person rather than many - upon downloading it it prompted me to install Java 7, which has reached its end of life.
                  I was, however, able to use Heritrix through the application to obtain a WARC file of the gallery website.</p>
                  <img src="tools-4.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="Screenshot of WAIL archiving a website">
                  <br><p>Having a GUI made it simple and straightforward to grab the file, but upon replay not all of the links I would have wanted to save had been captured.
                    There are ways to monitor your crawl while it’s happening (under the 'advanced' tab), but I was unable to figure out a way to control the scope through this application.</p>
                </div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-md-12">
                  <hr class="pink">
                  <h5>Squidwarc</h5>
                  </div>
                </div>
                <div class="row">
                  <div class="col-md-6 vertical-center">
                    <p>I am still a beginner when it comes to using the command line, but wanted to explore one of the more advanced option for obtaining WARC files.
                      With technical assistance (many thanks to <a href="https://www.linkedin.com/in/henryhsnow" target="_blank">Henry Snow</a>!), I tested out Squidwarc.
                      <a href="https://github.com/N0taN3rd/Squidwarc" target="_blank">Squidwarc</a> is an archival crawler alternative to Heritrix that aims to be slightly easier for the personal archivist to use.
                      Like brozzler, it remotely controls a browser to crawl through pages.</p>
                      <p>To install and use you must be running macOS or Linux, and be comfortable installing and running programs using the command-line.
                      Because it is a command-line based tool, you can easily schedule regular crawls once you’ve defined them.</p>
                    <p>Once you’ve installed the software, you can prompt it to open a browser window by executing the “run-chrome.sh” script.
                      In a new terminal window, you can open your config file and indicate whether you’d like to crawl just one page, the page and all same domain links, or the page and all links.
                      For my first capture, I chose a depth of 1, which tells the crawler to capture files one link away from the main seed.
                      As I thought this would not be sufficient to fully capture the site, I gave my next crawl a depth of 3.</p>
                      <p>Once you’ve edited your scope, you can execute the “run-crawler.sh” script, and the program will crawl through the site indicated within the parameters you set in your config file.</p>
                    </div>
                    <div class="col-md-6 vertical-center">
                      <div class="text-center">
                      <img src="tools-5.png" class="figure-img img-responsive img-thumbnail img-fluid" alt="Screenshot of config file">
                      <figcaption class="figure-caption">My first config file with a depth of 1</figcaption><br></div>
                    </div>
                  </div>
                </div>
                <div class="container">
                  <div class="row">
                    <div class="col-md-12 text-center">
                      <video src="Screencast-1.m4v" class="figure-img img-fluid img-thumbnail img-responsive" autoplay muted loop alt="Screen capture of a terminal window with the Squidwarc crawler script running"></video>
                      <figcaption class="figure-caption">Squidwarc crawling through many more seeds during my second capture</figcaption><br>
                    </div>
                  </div>
                </div>
                <div class="container">
                  <div class="row">
                    <div class="col-md-12 text-center">
                      <video src="Screencast-2.m4v" class="figure-img img-fluid img-thumbnail img-responsive" autoplay muted loop alt="Screen capture of Squidwarc scrolling through a browser window"></video>
                      <figcaption class="figure-caption">You can watch browser based crawlers navigate through each page!</figcaption>
                  </div>
                </div>
              </div>
              <div class="container">
                <div class="row">
                  <div class="col-md-12">
                <br><p>The second capture took significantly longer as the crawler had much more to process.
                  In the end I obtained WARC files of the site, each reflecting the different scope I outlined.
                  My first crawl, with the depth of 1, produced an 18 MB WARC file, while the second crawl produced a 323.2 MB WARC file.</p>
                </div>
              </div>
            </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <h3>Playing Back</h3>
        <hr class="pink">
        <h5>Wayback Machine</h5>
        <p>Most options for playing back WARC files are some version of the Wayback Machine.
          Pages saved using Archive-It can be viewed through the Wayback Machine.
          Links to other captures in the Internet Archive of the website are linked to in the top header.</p>
        <hr class="pink">
        <h5>Webrecorder player</h5>
        <hr class="pink">
        <h5>Oldweb.today</h5>
        <hr class="pink">
        <h5>More resources to explore</h5>
        <p>These are just a few of many tools you can use to acquire and replay websites, not to mention the many storage, metadata, and automation options that are currently being developed.
        If you are tech-savvy, you can explore many more advanced, command-line based tools, or even modify open-source software to create your own custom tools.</p>
        <ul>
          <li>For an incredibly thorough comparison of web archiving software, check out this <a href="https://github.com/datatogether/research/tree/master/web_archiving" target="_blank">Google spreadsheet</a> on github</li>
          <li>To learn more about web archiving in general, the <a href="https://github.com/iipc/awesome-web-archiving" target="_blank">awesome web archiving list</a> is (as its name implies) a truly awesome collection of collectively updated resources, and the best place to look to discover and contribute to different tools</li>
          <li>If you’re looking to use bleeding edge tools rather than using the dominant crawlers put forth by the Internet Archive, test them out using Mat Kelly's <a href="http://acid.matkelly.com/" target="_blank">Archival Acid Test</a></li>
        </ul>
      </div>
    </div>
  </div>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <h3>Conclusions</h3>
        <hr class="pink">
        <p>For someone with fairly limited technical experience, experimenting with web archiving proved to be a frustrating experience overall. I can understand why people pay for technical support and the convenience of built-in storage and access.
          Should you decide to use some of the more advanced free options like Squidwarc, you should be comfortable using the command-line and have a robust and collaborative relationship with your IT department.</p>
          <p>The free tools I explored were really created to support personal archiving, and may not scale well to the institutional level depending on how much you want to save.
            Additionally, I faced some problems with the smaller open-source software projects, as some had lost momentum and were not updating as fast as Chrome or Java.
            Cutting-edge projects can provide you with alternative methods for capturing WARC files if tools like Heritrix are not sufficient for your purposes, but it is hard to know how viable tools like WARCreate, WAIL, or Squidwarc will be in the future.</p>
            <p>That’s not to say that all open-source software is unviable - the tools developed by the Internet Archive are resoundingly successful as there is a large buy-in and a committed developer community working with these tools and suggesting updates as necessary.</p>
            <p>If you are planning a large-scale, long-term, or potentially difficult project, the support of Archive-It will probably make it your best option.
              I may have been able to acquire and play back pages, but figuring out quality assurance, automation and streamlining of the process, storage and maintenance, and how to provide access would have been difficult.
              It would likely involve cobbling together and maintaining multiple tools and programs.
              Archive-It provides access alongside other potentially similar collections, stores copies of your data for you, and all steps of the process are essentially contained within one service.
              Its biggest advantage over the simple browser based options? - YOU have fine-grained control over scoping, and once you’ve set up a crawl, you can repeat it at a certain frequency.</p>
            <p>As an alternative, I found Webrecorder to be a fantastic and easy way to archive.
              While it’s not as automated as crawler-based options, it offers control over the scope, a simple framework for collection organization, and access to raw data.
              It also avoids many of the common pitfalls associated with web archiving, and I was easily able to navigate it without technical assistance.</p>
      </div>
    </div>
    </div>
</body>
<footer>
    <div class="footer">
    <p class="by">Created by Chelsea Cates & Abigail Walker, 2018</p>
    </div>
</footer>
</html>
